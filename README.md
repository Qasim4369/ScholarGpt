ScholarGPT â€“ Religious Text Comparison using RAG

ScholarGPT is a Retrieval-Augmented Generation (RAG) pipeline built in Python that compares religious texts (Islam, Christianity, Judaism) on specific topics. It retrieves relevant verses, augments them into a prompt, and uses a generative model to create a comparative answer.

ğŸš€ Features

ğŸ“‚ Data Collection â†’ Stores religious texts in JSON format.

ğŸ§  Embeddings & Indexing â†’ Uses FAISS for semantic search.

ğŸ” Similarity Search â†’ Finds relevant verses for a given query.

ğŸ“ Prompt Augmentation â†’ Builds a structured prompt with retrieved passages.

ğŸ¤– Text Generation â†’ Uses FLAN-T5 / Open Source Models for comparative answers.

ğŸ’» Interactive Mode â†’ Ask a question directly in the terminal.

âš™ï¸ Project Structure
ScholarGPT/
â”‚â”€â”€ collect_online.py      # Collects and stores texts in JSON
â”‚â”€â”€ embedding.py           # Generates embeddings & builds FAISS index
â”‚â”€â”€ query.py               # Tests similarity search
â”‚â”€â”€ augmentation.py        # Builds augmented prompt
â”‚â”€â”€ generation.py          # Generates final comparative answer
â”‚â”€â”€ final_pipeline.py      # Orchestrates augmentation + generation
â”‚â”€â”€ data.json              # Religious texts dataset
â”‚â”€â”€ augmented_prompt.txt   # Generated prompt for LLM
â”‚â”€â”€ final_comparison.txt   # Final generated answer
â”‚â”€â”€ README.md              # Project documentation

ğŸ› ï¸ Installation

Clone the repository:

git clone https://github.com/yourusername/ScholarGPT.git
cd ScholarGPT


Create a virtual environment (recommended):

python -m venv venv
source venv/bin/activate  # Linux / Mac
venv\Scripts\activate     # Windows


Install dependencies:

pip install -r requirements.txt

â–¶ï¸ Usage
Run the pipeline (without recollecting/embedding again):
python final_pipeline.py


Youâ€™ll be prompted to enter a query, e.g.:

Enter your query: What do Islam, Christianity, and Judaism say about wine?


âœ… ScholarGPT will:

Retrieve relevant verses

Build an augmented prompt

Generate a comparative answer

Output will be saved in:

augmented_prompt.txt

final_comparison.txt

ğŸ¤– Models

Currently uses:

FLAN-T5
 (base, free, ~512 tokens)

Easily swappable with larger models (e.g. flan-t5-large, mistral-7b-instruct, etc.)

ğŸ“Œ Example Output

Query: What do Islam, Christianity, and Judaism say about wine?

--- Islam ---
Quran 83:25 â€” "Their thirst will be slaked with Pure Wine sealed:"
Quran 76:17 â€” "And they will be given to drink there of a Cup (of Wine) mixed with Zanjabil,-"

--- Christianity ---
John 2:10 â€” "Every man at the beginning doth set forth good wine..."

--- Judaism ---
Genesis 19:32 â€” "Come, let us make our father drink wine..."

ğŸ›¤ï¸ Roadmap

 Add UI with Streamlit/Gradio for web interaction

 Improve dataset coverage (more scriptures, translations)

 Switch to long-context models (Mistral, Llama-3, etc.)

 Deploy as an API or Web App

ğŸ¤ Contributing

Pull requests are welcome! If youâ€™d like to add new features or improve datasets, open an issue first to discuss what youâ€™d like to change.

ğŸ“œ License

MIT License â€“ feel free to use, modify, and share.
